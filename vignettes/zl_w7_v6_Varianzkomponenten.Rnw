%  Example for the ETH Beamer template
%  Copyright 2014 by
%  Dr. Antonios Garas,
%  Chair of Systems Design, ETH Zurich
%  Weinbergstrasse 56/58 CH-8092 Zurich
%
\pdfminorversion=4
\documentclass[
%aspectratio=169,
first,
%handout,
%compress,
%Helv,
ETH3, %ETH1,
navigation
]{ETHbeamerclass}

% Options for beamer:
%
% compress: navigation bar becomes smaller
% t       : place contents of frames on top (alternative: b,c)
% handout : handoutversion
% notes   : show notes
% notes=onlyslideswithnotes
%
\setbeamertemplate{note page}{\ \\[.3cm]
\textbf{\color{colorSG}Notes:}\\%[0.1cm]
{\footnotesize %\tiny
\insertnote}}

\setbeameroption{hide notes}
%\setbeameroption{show notes}

\setbeamertemplate{navigation symbols}{} % suppresses all navigation symbols:
% \setbeamertemplate{navigation symbols}[horizontal] % Organizes the navigation symbols horizontally.
% \setbeamertemplate{navigation symbols}[vertical] % Organizes the navigation symbols vertically.
% \setbeamertemplate{navigation symbols}[only frame symbol] % Shows only the navigational symbol for navigating frames.

\usepackage{amsmath, amssymb}
\usepackage{pifont}
\usepackage{multimedia}

%--------------------------------------------------------------------------------------------------------
%--- Some custom definitions...

\definecolor{cbf}{RGB}{199,100,95}%{163,15,0}
\newcommand{\cbf}{\color{cbf}}

\newcommand{\mean}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left| #1 \right|}

%--------------------------------------------------------------------------------------------------------
%--- Inputs for the title page...

\title{Varianzkomponentensch\"atzung}
\newcommand{\shorttitle}{}

\author{Peter von Rohr}
\date{\today}
\institute{Qualitas AG}

\newcommand{\collaborators}{}
\newcommand{\event}{Folien ZL I+II}
\newcommand{\place}{LFW C11}

\newcommand{\Prob}{\mathrm{Pr}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\nbin}[1]{\mathrm{nb}_\mathrm{in}(#1)}
\newcommand{\nbout}[1]{\mathrm{nb}_\mathrm{out}(#1)}
\newcommand{\one}{\mathbb{\bf 1}}

\newcommand{\sref}[1]{SLIDE \ref{#1}}

%--------------------------------------------------------------------------------------------------------
%--- Here begins the presentation...

\begin{document}
\SweaveOpts{concordance=TRUE}

\frame{
\maketitle
}
\note{}


\section{Varianzkomponentensch\"atzung}

\frame{
  \frametitle{Multiple Lineare Regression}
  \begin{block}{Annahmen}
    \begin{itemize}
    \item Modell $$\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e}$$
    \item Varianz der Fehler $e_i$ ist konstant, d.h. $Var(e_i) = \sigma^2$ f\"ur alle $i$
    \item Fehler $e_i$ sind nicht korreliert, d.h. $Cov(e_i,e_j) = 0$ f\"ur $i\ne j$
    \item Fehler $\mathbf{e}$ ist multivariat normal verteilt
    \end{itemize}
  \end{block}
  \begin{block}{Momente}
    \begin{itemize}
    \item Erwartungswerte: $E\left[\mathbf{e} \right] = \mathbf{0}$, $E\left[\mathbf{y} = \mathbf{X}\mathbf{b} \right]$
    \item Varianzen: $Var(y_i) = \sigma^2$, $Var(e_i) = \sigma^2$ f\"ur alle $i$
    \end{itemize}
  \end{block}
}
\note{}


\frame{
  \frametitle{Parametersch\"atzung}
  \begin{itemize}
  \item Im linearen Model $\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e}$ sind $\mathbf{b}$ und $\sigma^2$ unbekannte Parameter
  \item Parameter sollen als Funktion der Daten ($\mathbf{y}$) gesch\"atzt werden
  \item Ein m\"oglicher Sch\"atzer $\hat{b}_{LS}$ f\"ur $\mathbf{b}$ kann mit \textbf{Least Squares} (kleinste Quadrate) berechnet werden
  $$\hat{\mathbf{b}}_{LS} = \left(\mathbf{X}^T\mathbf{X} \right)^{-1}\mathbf{X}^T\mathbf{y}$$
  \item \textbf{Aber} Least Squares gibt keine Sch\"atzung f\"ur $\sigma^2$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Sch\"atzung der Varianzkomonente $\sigma^2$}
  Eine Beobachtung betreffend der Eigenschaft der \textbf{Erwartungstreue} (Unbiasedness) eines Sch\"atzers f\"uhrt zu folgendem Ergebnis
  \begin{itemize}
  \item Erwartungstreue (Unbiasedness) ist eine Eigenschaft eines Sch\"atzers $\hat{\theta}$ f\"ur einen Parameter $\theta$, sofern gilt, dass $E\left[\hat{\theta} \right] = \theta$
  \item Erwartungswert der Summe der quadrierten Residuen ($r_i$)
  $$E\left[\sum_{i=1}^n r_i^2 \right]
    =  \sum_{i=1}^n E\left[r_i^2 \right]
    =  \sum_{i=1}^n Var(r_i)
    =  \sigma^2 \sum_{i=1}^n (1 - P_{ii})$$
  $$
    =  \sigma^2 (n-tr(P))
    =  \sigma^2 (n-p)$$
    \noindent wobei: $r_i = y_i - \hat{y}_i = y_i - \mathbf{x}_i^T\hat{b}$
      und Matrix $\mathbf{P} = \mathbf{X}\left(\mathbf{X}^T\mathbf{X} \right)^{-1}\mathbf{X}^T$,\\
      $n$: Anzahl Beobachtungen und $p$: Anzahl Parameter
  \end{itemize}


}
\note{}

\frame{
  \frametitle{Sch\"atzung der Varianzkomonente $\sigma^2$ II}
    \begin{itemize}
    \item Somit gilt Erwartungstreue f\"ur den Sch\"atzer
      $$\hat{\sigma^2} =  {1\over (n-p)} \sum_{i=1}^n r_i^2$$
    \item da f\"ur die Erwartungswerte gilt
      $$E\left[\hat{\sigma^2} \right] = E\left[{1\over (n-p)} \sum_{i=1}^n r_i^2 \right]
         = {1\over (n-p)} E\left[\sum_{i=1}^n r_i^2 \right]$$
      $$ = {1\over (n-p)} \left[ \sigma^2 (n-p)\right] = \sigma^2$$
    \item Dieser Sch\"atzer wird oft als LS-Sch\"atzer bezeichnet
    \end{itemize}
}
\note{}


\begin{frame}
  \frametitle{Varianzanalyse - ANOVA}
  \begin{itemize}
  \item Effekte oder Faktoren im Modell (bis jetzt $x$ Variable) sind kategorisch oder qualitativ
  \item Ziel ist es herauszufinden, wie viel von der totalen empirischen Varianz in den Beobachtungen durch die Faktoren erkl\"art werden kann.
  \item Folgendes Modell wird angenommen:
    $$y_{ij} = \mu + \alpha_i + e_{ij} $$
    \noindent wobei: Faktor $\alpha$ $i=1,...,I$ Stufen aufweist und $j=1,...,J_i$ Beobachtungen pro Stufe vorliegen.
  \item Effekte sind nur unter gewissen Restriktionen sch\"atzbar. H\"aufig wird die Summe aller Effekte $\sum_i J_i \alpha_i = 0$ gesetzt
  \end{itemize}
\end{frame}

\frame{
  \frametitle{Parametersch\"atzung ANOVA}
  \begin{itemize}
  \item Mit Restriktion $\sum_i J_i \alpha_i = 0$ folgen Sch\"atzer
    $$\hat{\mu} = \bar{y} \text{ und } \hat{\alpha}_i = \bar{y}_{i.} - \bar{y}$$
    \noindent wobei: $\bar{y}$ der Mittelwert \"uber alle Beobachtungen und $\bar{y}_{i.}$ der Mittelwert \"uber alle Beobachtungen auf Stufe $i$ ist.
  \item Die Varianzanteile werden aufgrund der Summenquadrate bestimmt
  \end{itemize}
}

\frame{
  \frametitle{Tabelle der Varianzanalyse - ANOVA Table}
  \begin{tabular}{lrrrrr}
  \hline
  Quelle           &  DF     &  SQ            &  MSQ   &  F         &  P  \\
  \hline
  Faktor $\alpha$  &  $i-1$  &  SQZ           &  MSQZ  &  MSQZ$/$MSQE &   \\
  Fehler $e$       &  $n-i$  &  SQE           &  MSQE  &              &   \\
  Total            &  $n-1$  &  SQT           &        &              &   \\
  \hline
  \end{tabular}

  \noindent wobei
  $$SQT = \sum_{i=1}^I\sum_{j=1}^{J_i}(y_{ij} - \bar{y})^2$$
  $$SQZ = \sum_{i=1}^I\sum_{j=1}^{J_i}(\bar{y}_{i.} - \bar{y})^2 \text{; } MSQZ = SQZ/(i-1)$$
  $$SQE = \sum_{i=1}^I\sum_{j=1}^{J_i}(y_{ij} - \bar{y}_{i.})^2 \text{; } MSQE = SQE/(n-i)$$
}

\begin{frame}[fragile]
  \frametitle{Beispieldatensatz}
  Koagulationsdatensatz von \verb+https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf+
<<CoagData>>=
if (!require(faraway)) {
  install.packages("faraway");require(faraway)}
data("coagulation")
head(coagulation, n=5)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Beispieldatensatz II}
<<CoagSummary>>=
summary(coagulation)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{Beispieldatensatz Anova}
<<CoagAnova>>=
lmCoDi <- lm(coag ~ diet, data = coagulation)
anova(lmCoDi)
@
\end{frame}

\frame{
  \frametitle{Maximum Likelihood}
    \begin{itemize}
    \item Definition der Likelihood als gemeinsame Dichteverteilung der Daten ($\mathbf{y}$) gegeben die Parameter ($\mathbf{b}$, $\sigma^2$)
      $$ Pr\left(\mathbf{y} | \mathbf{b},\sigma^2\right)$$
    \item Unter der Annahme einer Normalverteilung und nach Beobachtung der Daten als Funktion der Parameter
          aufgefasst, folgt
      $$ L\left(\mathbf{b},\sigma^2;\mathbf{y}\right) = Pr\left(\mathbf{y} | \mathbf{b},\sigma^2\right)
         = (2\pi\sigma^2)^{-{n\over2}}
           \prod_{i=1}^n exp \left[-\frac{(y_i - \mathbf{x}_i^T\mathbf{b})^2}{2\sigma^2} \right]
      $$
    \end{itemize}
}
\note{}

\frame{
  \frametitle{Maximierung der Likelihood Funktion}
  \begin{itemize}
  \item Maximierung der Likelihood Funktion $L\left(\mathbf{b},\sigma^2;\mathbf{y}\right)$ ist
    analog zur Maximierung von
    $$l\left(\mathbf{b},\sigma^2;\mathbf{y}\right)
      = \log\left(L\left(\mathbf{b},\sigma^2;\mathbf{y}\right) \right)$$
    $$ = -{n\over2} \log(2\pi\sigma^2) - {1\over 2\sigma^2}
      \sum_{i=1}^n (y_i - \mathbf{x}_i^T\mathbf{b})^2$$
  \item Sch\"atzungen f\"ur $\mathbf{b}$ und $\sigma$ erhalten wir durch Nullsetzen der partiellen
    Ableitungen von $l\left(\mathbf{b},\sigma^2;\mathbf{y}\right)$ nach $\mathbf{b}$ und $\sigma$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Partielle Ableitung nach $\mathbf{b}$}
  \begin{itemize}
  \item Partielle Ableitung von $l\left(\mathbf{b},\sigma^2;\mathbf{y}\right)$ nach $\mathbf{b}$
    $$\frac{\partial l}{\partial \mathbf{b}} =  - {1\over 2\sigma^2}
      \sum_{i=1}^n 2\mathbf{x}_i(y_i - \mathbf{x}_i^T\mathbf{b})$$
  \item Extremum ist erreicht, falls partielle Ableitung $=0$, somit ist
    $$ \sum_{i=1}^n \mathbf{x}_i\mathbf{x}_i^T\hat{\mathbf{b}} = \sum_{i=1}^n \mathbf{x}_iy_i$$
    $$ \hat{\mathbf{b}}_{ML} = \left(\sum_{i=1}^n \mathbf{x}_i\mathbf{x}_i^T \right)^{-1}\sum_{i=1}^n \mathbf{x}_iy_i$$
  \item Beobachtung: $\hat{\mathbf{b}}_{LS} = \hat{\mathbf{b}}_{ML}$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Sch\"atzer f\"ur $\sigma^2$}
  \begin{itemize}
  \item Analoges Vorgehen, wie f\"ur $\mathbf{b}$
  \item Partielle Ableitung von $l\left(\mathbf{b},\sigma^2;\mathbf{y}\right)$ nach $\sigma^2$ null setzen
  \item F\"ur $\mathbf{b}$ setzen wir $\hat{\mathbf{b}}_{ML}$ ein
    $$\frac{\partial l}{\partial \sigma^2} = -{n\over 2\sigma^2} + {1\over 2\sigma^4}
      \sum_{i=1}^n (y_i - \mathbf{x}_i^T\mathbf{b})^2 $$
    $$ n \hat{\sigma^2}_{ML} = \sum_{i=1}^n (y_i - \mathbf{x}_i^T\hat{\mathbf{b}}_{ML})^2$$
    $$ \hat{\sigma^2}_{ML} = {1\over n}\sum_{i=1}^n (y_i - \mathbf{x}_i^T\hat{\mathbf{b}}_{ML})^2$$
  \item $\hat{\sigma^2}_{ML}$ ist nicht erwartungstreu, d.h. $E\left[\hat{\sigma^2}_{ML} \right] \ne \sigma^2$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{REML}
  \begin{itemize}
  \item Restricted oder Residual Maximum Likelihood
  \item Basiert auf der Log-likelihood Funktion
    $$ l_{REML}(\sigma^2;\mathbf{y}) = -\frac{n-p}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^nr_i^2$$
    \noindent wobei:
    $$ r_i = y_i - \mathbf{x}_i^T \left(\sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T \right)^{-1} \sum_{i=1}^n \mathbf{x}_i y_i$$
  \item $l_{REML}$ ist unabh\"angig von $\mathbf{b}$
  \item Maximierung von $l_{REML}$ f\"uhrt zum Sch\"atzer
    $$\hat{\sigma^2}_{REML} = \frac{1}{n-p} \sum_{i=1}^nr_i^2$$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Bayes'sche Methoden}
  \begin{block}{Philosophie}
    \begin{itemize}
    \item Welt der Statistik unterteilt sich in zwei Lager
      \begin{enumerate}
      \item Frequentisten
      \item Bayesianer
      \end{enumerate}
    \end{itemize}
  \end{block}
  \begin{block}{Frequentisten}
    \begin{itemize}
    \item Parametersch\"atzungen basieren auf ML oder REML
    \item Trennung zwischen Daten und Parameter, fehlende Daten werden ignoriert
    \item Effekte in Modellen werden in fix und zuf\"allig unterteilt
    \item Keine Ber\"ucksichtigung von a priori Information
    \end{itemize}
  \end{block}
}
\note{}

\frame{
  \frametitle{Bayes'sche Methoden II}
   \begin{block}{Bayesianer}
    \begin{itemize}
    \item Unterteilung in bekannte und unbekannte Gr\"ossen, dies k\"onnen Daten oder Parameter sein
    \item Sch\"atzung von unbekannten Gr\"ossen basieren auf der a posteriori Verteilung der unbekannten Gr\"ossen gegeben die bekannten Gr\"ossen
    \item Fehlende Daten k\"onnen ber\"ucksichtigt werden
    \item A priori Information wird ber\"ucksichtigt
    \end{itemize}
  \end{block}
}
\note{}

\frame{
  \frametitle{Beispiel f\"ur Bayes'sche Parametersch\"atzung}
  \begin{itemize}
  \item Modell: $\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e}$
  \item Bekannte Gr\"ossen sind die beobachteten Daten $\mathbf{y}$
  \item Unbekannte Gr\"ossen sind die Parameter $\mathbf{b}$ und $\sigma$
  \item Sch\"atzungen f\"ur Unbekannte $\mathbf{b}$ und $\sigma$ werden gemacht
    aufgrund der a posteriori Dichteverteilungen $f(\mathbf{b} | \mathbf{y},\sigma^2)$ und
    $f(\sigma^2 | \mathbf{y}, \mathbf{b})$
  \item A posteriori Dichteverteilungen werden aufgrund des Satzes von Bayes berechnet
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Satz von Bayes}
  $$ f(\mathbf{b} | \mathbf{y},\sigma^2)
     = \frac{f(\mathbf{y} |\mathbf{b},\sigma^2)f(\mathbf{b})f(\sigma^2)}{f(\mathbf{y},\sigma^2)}$$
  \noindent wobei

  \vspace{2ex}
  \begin{tabular}{ll}
  $f(\mathbf{y} |\mathbf{b},\sigma^2)$  &  Bayes'sche Likelihood \\
  $f(\mathbf{b})$                       &  a priori Dichteverteilung von $\mathbf{b}$ \\
  $f(\sigma^2)$                         &  a priori Dichteverteilung von $\sigma^2$ \\
  $f(\mathbf{y},\sigma^2)$              &  Normalisierungskonstante \\
  \end{tabular}
}


\frame{
  \frametitle{Bayes'sche Sch\"atzung f\"ur $\mathbf{b}$}
  \begin{itemize}
  \item F\"ur Sch\"atzung werden nur die von $\mathbf{b}$ abh\"angigen Teile aus
    $f(\mathbf{b} | \mathbf{y},\sigma^2)$ verwendet
  \item Konstante $f(\sigma^2)$ und $f(\mathbf{y},\sigma^2)$ werden ignoriert
  \item Daraus folgt
    $$ f(\mathbf{b} | \mathbf{y},\sigma^2) \propto f(\mathbf{y} |\mathbf{b},\sigma^2)f(\mathbf{b})$$
  \item Ist a priori nichts \"uber $\mathbf{b}$ bekannt, dann wird $f(\mathbf{b})$ als konstant angenommen
    (uninformativer Prior)
  \item Weitere Vereinfachung zu
    $$ f(\mathbf{b} | \mathbf{y},\sigma^2) \propto f(\mathbf{y} |\mathbf{b},\sigma^2)$$
  \end{itemize}
}
\note{}


\frame{
  \frametitle{Bayes'sche Sch\"atzung f\"ur $\mathbf{b}$ II}
  \begin{itemize}
  \item Unter der Annahme der Normalverteilung folgt
    $$ f(\mathbf{y} |\mathbf{b},\sigma^2)
      = (2\pi\sigma^2)^{-{n\over 2}} exp\left\{
        - \frac{(\mathbf{y} - \mathbf{X}\mathbf{b})^T(\mathbf{y} - \mathbf{X}\mathbf{b})}
             {2\sigma^2} \right\}$$
  \item Damit die Dimensionen konsistent sind, machen wir folgende Transformation
    $$ f(\mathbf{b} | \mathbf{y},\sigma^2) \propto
       exp\left\{- \frac{(\mathbf{y} - \mathbf{X}\mathbf{b})^T(\mathbf{y} - \mathbf{X}\mathbf{b})}
                        {2\sigma^2} \right\}$$
    $$ \propto exp\left\{ -\frac{(\mathbf{b}-\hat{\mathbf{b}})^T(\mathbf{X}^T\mathbf{X})(\mathbf{b}-\hat{\mathbf{b}})}{2\sigma^2}\right\}$$
    \noindent wobei: $\hat{\mathbf{b}} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}$
  \end{itemize}

}
\note{}


\frame{
  \frametitle{Bayes'sche Sch\"atzung III}
  \begin{itemize}
  \item Sch\"atzung f\"ur $\sigma^2$ analog basierend auf $f(\sigma^2 | \mathbf{y},\mathbf{b})$
  \item Sch\"atzwerte basieren auf Momenten (Erwartungswert und Varianz) der a posteriori Dichteverteilungen
  \item Entsprechen die a posteriori Dichteverteilungen keinen Standarddichteverteilungen (wie Normal, Student-t, Beta, Gamma, ...), dann werden Erwartungswert und Varianzen aufgrund von Zufallszahlen von diesen Dichten approximiert
  \item Zufallszahlen werden aufgrund von Markovketten generiert
  \end{itemize}
}
\note{}


\end{document}
