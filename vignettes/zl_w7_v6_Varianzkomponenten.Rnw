%  Example for the ETH Beamer template
%  Copyright 2014 by
%  Dr. Antonios Garas,
%  Chair of Systems Design, ETH Zurich
%  Weinbergstrasse 56/58 CH-8092 Zurich
%
\pdfminorversion=4
\documentclass[
%aspectratio=169,
first,
%handout,
%compress,
%Helv,
ETH3, %ETH1,
navigation
]{ETHbeamerclass}

% Options for beamer:
%
% compress: navigation bar becomes smaller
% t       : place contents of frames on top (alternative: b,c)
% handout : handoutversion
% notes   : show notes
% notes=onlyslideswithnotes
%
\setbeamertemplate{note page}{\ \\[.3cm]
\textbf{\color{colorSG}Notes:}\\%[0.1cm]
{\footnotesize %\tiny
\insertnote}}

\setbeameroption{hide notes}
%\setbeameroption{show notes}

\setbeamertemplate{navigation symbols}{} % suppresses all navigation symbols:
% \setbeamertemplate{navigation symbols}[horizontal] % Organizes the navigation symbols horizontally.
% \setbeamertemplate{navigation symbols}[vertical] % Organizes the navigation symbols vertically.
% \setbeamertemplate{navigation symbols}[only frame symbol] % Shows only the navigational symbol for navigating frames.

\usepackage{amsmath, amssymb}
\usepackage{pifont}
\usepackage{multimedia}

%--------------------------------------------------------------------------------------------------------
%--- Some custom definitions...

\definecolor{cbf}{RGB}{199,100,95}%{163,15,0}
\newcommand{\cbf}{\color{cbf}}

\newcommand{\mean}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left| #1 \right|}

%--------------------------------------------------------------------------------------------------------
%--- Inputs for the title page...

\title{Varianzkomponentensch\"atzung}
\newcommand{\shorttitle}{}

\author{Peter von Rohr}
\date{\today}
\institute{Qualitas AG}

\newcommand{\collaborators}{}
\newcommand{\event}{Folien ZL I+II}
\newcommand{\place}{LFW C11}

\newcommand{\Prob}{\mathrm{Pr}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\nbin}[1]{\mathrm{nb}_\mathrm{in}(#1)}
\newcommand{\nbout}[1]{\mathrm{nb}_\mathrm{out}(#1)}
\newcommand{\one}{\mathbb{\bf 1}}

\newcommand{\sref}[1]{SLIDE \ref{#1}}

%--------------------------------------------------------------------------------------------------------
%--- Here begins the presentation...

\begin{document}
\SweaveOpts{concordance=TRUE}

\frame{
\maketitle
}
\note{}


\section{Varianzkomponentensch\"atzung}

\frame{
  \frametitle{Multiple Lineare Regression}
  \begin{block}{Annahmen}
    \begin{itemize}
    \item Modell $$\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e}$$
    \item Varianz der Fehler $e_i$ ist konstant, d.h. $Var(e_i) = \sigma^2$ f\"ur alle $i$
    \item Fehler $e_i$ sind nicht korreliert, d.h. $Cov(e_i,e_j) = 0$ f\"ur $i\ne j$
    \item Fehler $\mathbf{e}$ ist multivariat normal verteilt
    \end{itemize}
  \end{block}
  \begin{block}{Momente}
    \begin{itemize}
    \item Erwartungswerte: $E\left[\mathbf{e} \right] = \mathbf{0}$, $E\left[\mathbf{y} = \mathbf{X}\mathbf{b} \right]$
    \item Varianzen: $Var(y_i) = \sigma^2$, $Var(e_i) = \sigma^2$ f\"ur alle $i$
    \end{itemize}
  \end{block}
}
\note{}


\frame{
  \frametitle{Parametersch\"atzung}
  \begin{itemize}
  \item Im linearen Model $\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e}$ sind $\mathbf{b}$ und $\sigma^2$ unbekannte Parameter
  \item Parameter sollen als Funktion der Daten ($\mathbf{y}$) gesch\"atzt werden
  \item Ein m\"oglicher Sch\"atzer $\hat{b}_{LS}$ f\"ur $\mathbf{b}$ kann mit \textbf{Least Squares} (kleinste Quadrate) berechnet werden
  $$\hat{\mathbf{b}}_{LS} = \left(\mathbf{X}^T\mathbf{X} \right)^{-1}\mathbf{X}^T\mathbf{y}$$
  \item \textbf{Aber} Least Squares gibt keine Sch\"atzung f\"ur $\sigma^2$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Sch\"atzung der Varianzkomonente $\sigma^2$}
  Eine Beobachtung betreffend der Eigenschaft der \textbf{Erwartungstreue} (Unbiasedness) eines Sch\"atzers f\"uhrt zu folgendem Ergebnis
  \begin{itemize}
  \item Erwartungstreue (Unbiasedness) ist eine Eigenschaft eines Sch\"atzers $\hat{\theta}$ f\"ur einen Parameter $\theta$, sofern gilt, dass $E\left[\hat{\theta} \right] = \theta$
  \item Erwartungswert der Summe der quadrierten Residuen ($r_i$)
  $$E\left[\sum_{i=1}^n r_i^2 \right]
    =  \sum_{i=1}^n E\left[r_i^2 \right]
    =  \sum_{i=1}^n Var(r_i)
    =  \sigma^2 \sum_{i=1}^n (1 - P_{ii})$$
  $$
    =  \sigma^2 (n-tr(P))
    =  \sigma^2 (n-p)$$
    \noindent wobei: $r_i = y_i - \hat{y}_i = y_i - \mathbf{x}_i^T\hat{b}$
      und Matrix $\mathbf{P} = \mathbf{X}\left(\mathbf{X}^T\mathbf{X} \right)^{-1}\mathbf{X}^T$,\\
      $n$: Anzahl Beobachtungen und $p$: Anzahl Parameter
  \end{itemize}


}
\note{}

\frame{
  \frametitle{Sch\"atzung der Varianzkomonente $\sigma^2$ II}
    \begin{itemize}
    \item Somit gilt Erwartungstreue f\"ur den Sch\"atzer
      $$\hat{\sigma^2} =  {1\over (n-p)} \sum_{i=1}^n r_i^2$$
    \item da f\"ur die Erwartungswerte gilt
      $$E\left[\hat{\sigma^2} \right] = E\left[{1\over (n-p)} \sum_{i=1}^n r_i^2 \right]
         = {1\over (n-p)} E\left[\sum_{i=1}^n r_i^2 \right]$$
      $$ = {1\over (n-p)} \left[ \sigma^2 (n-p)\right] = \sigma^2$$
    \item Dieser Sch\"atzer wird oft als LS-Sch\"atzer bezeichnet
    \end{itemize}
}
\note{}

\frame{
  \frametitle{Maximum Likelihood}
    \begin{itemize}
    \item Definition der Likelihood als gemeinsame Dichteverteilung der Daten ($\mathbf{y}$) gegeben die Parameter ($\mathbf{b}$, $\sigma^2$)
      $$ Pr\left(\mathbf{y} | \mathbf{b},\sigma^2\right)$$
    \item Unter der Annahme einer Normalverteilung und nach Beobachtung der Daten als Funktion der Parameter
          aufgefasst, folgt
      $$ L\left(\mathbf{b},\sigma^2;\mathbf{y}\right) = Pr\left(\mathbf{y} | \mathbf{b},\sigma^2\right)
         = (2\pi\sigma^2)^{-{n\over2}}
           \prod_{i=1}^n exp \left[-\frac{(y_i - \mathbf{x}_i^T\mathbf{b})^2}{2\sigma^2} \right]
      $$
    \end{itemize}
}
\note{}

\frame{
  \frametitle{Maximierung der Likelihood Funktion}
  \begin{itemize}
  \item Maximierung der Likelihood Funktion $L\left(\mathbf{b},\sigma^2;\mathbf{y}\right)$ ist
    analog zur Maximierung von
    $$l\left(\mathbf{b},\sigma^2;\mathbf{y}\right)
      = \log\left(L\left(\mathbf{b},\sigma^2;\mathbf{y}\right) \right)$$
    $$ = -{n\over2} \log(2\pi\sigma^2) - {1\over 2\sigma^2}
      \sum_{i=1}^n (y_i - \mathbf{x}_i^T\mathbf{b})^2$$
  \item Sch\"atzungen f\"ur $\mathbf{b}$ und $\sigma$ erhalten wir durch Nullsetzen der partiellen
    Ableitungen von $l\left(\mathbf{b},\sigma^2;\mathbf{y}\right)$ nach $\mathbf{b}$ und $\sigma$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Partielle Ableitung nach $\mathbf{b}$}
  \begin{itemize}
  \item Partielle Ableitung von $l\left(\mathbf{b},\sigma^2;\mathbf{y}\right)$ nach $\mathbf{b}$
    $$\frac{\partial l}{\partial \mathbf{b}} =  - {1\over 2\sigma^2}
      \sum_{i=1}^n 2\mathbf{x}_i(y_i - \mathbf{x}_i^T\mathbf{b})$$
  \item Extremum ist erreicht, falls partielle Ableitung $=0$, somit ist
    $$ \sum_{i=1}^n \mathbf{x}_i\mathbf{x}_i^T\hat{\mathbf{b}} = \sum_{i=1}^n \mathbf{x}_iy_i$$
    $$ \hat{\mathbf{b}}_{ML} = \left(\sum_{i=1}^n \mathbf{x}_i\mathbf{x}_i^T \right)^{-1}\sum_{i=1}^n \mathbf{x}_iy_i$$
  \item Beobachtung: $\hat{\mathbf{b}}_{LS} = \hat{\mathbf{b}}_{ML}$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Sch\"atzer f\"ur $\sigma^2$}
  \begin{itemize}
  \item Analoges Vorgehen, wie f\"ur $\mathbf{b}$
  \item Partielle Ableitung von $l\left(\mathbf{b},\sigma^2;\mathbf{y}\right)$ nach $\sigma^2$ null setzen
  \item F\"ur $\mathbf{b}$ setzen wir $\hat{\mathbf{b}}_{ML}$ ein
    $$\frac{\partial l}{\partial \sigma^2} = -{n\over 2\sigma^2} + {1\over 2\sigma^4}
      \sum_{i=1}^n (y_i - \mathbf{x}_i^T\mathbf{b})^2 $$
    $$ n \hat{\sigma^2}_{ML} = \sum_{i=1}^n (y_i - \mathbf{x}_i^T\hat{\mathbf{b}}_{ML})^2$$
    $$ \hat{\sigma^2}_{ML} = {1\over n}\sum_{i=1}^n (y_i - \mathbf{x}_i^T\hat{\mathbf{b}}_{ML})^2$$
  \item $\hat{\sigma^2}_{ML}$ ist nicht erwartungstreu, d.h. $E\left[\hat{\sigma^2}_{ML} \right] \ne \sigma^2$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{REML}
  \begin{itemize}
  \item Restricted oder Residual Maximum Likelihood
  \item Basiert auf der Log-likelihood Funktion
    $$ l_{REML}(\sigma^2;\mathbf{y}) = -\frac{n-p}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^nr_i^2$$
    \noindent wobei:
    $$ r_i = y_i - \mathbf{x}_i^T \left(\sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T \right)^{-1} \sum_{i=1}^n \mathbf{x}_i y_i$$
  \item $l_{REML}$ ist unabh\"angig von $\mathbf{b}$
  \item Maximierung von $l_{REML}$ f\"uhrt zum Sch\"atzer
    $$\hat{\sigma^2}_{REML} = \frac{1}{n-p} \sum_{i=1}^nr_i^2$$
  \end{itemize}
}
\note{}

\frame{
  \frametitle{Bayes'sche Methoden}
  \begin{block}{Philosophie}
    \begin{itemize}
    \item Welt der Statistik unterteilt sich in zwei Lager
      \begin{enumerate}
      \item Frequentisten
      \item Bayesianer
      \end{enumerate}
    \end{itemize}
  \end{block}
  \begin{block}{Frequentisten}
    \begin{itemize}
    \item Parametersch\"atzungen basieren auf ML oder REML
    \item Trennung zwischen Daten und Parameter, fehlende Daten werden ignoriert
    \item Effekte in Modellen werden in fix und zuf\"allig unterteilt
    \item Keine Ber\"ucksichtigung von a priori Information
    \end{itemize}
  \end{block}
}
\note{}

\frame{
  \frametitle{Bayes'sche Methoden II}
   \begin{block}{Bayesianer}
    \begin{itemize}
    \item Unterteilung in bekannte und unbekannte Gr\"ossen, dies k\"onnen Daten oder Parameter sein
    \item Sch\"atzung von unbekannten Gr\"ossen basieren auf der a posteriori Verteilung der unbekannten Gr\"ossen gegeben die bekannten Gr\"ossen
    \item Fehlende Daten k\"onnen ber\"ucksichtigt werden
    \item A priori Information wird ber\"ucksichtigt
    \end{itemize}
  \end{block}
}
\note{}

\frame{
  \frametitle{Beispiel f\"ur Bayes'sche Parametersch\"atzung}
  \begin{itemize}
  \item Modell: $\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e}$
  \end{itemize}
}
\note{}


\frame{
  \frametitle{Folientitel}
  \begin{block}{Blocktitel}

  \end{block}
}
\note{}


\section{Selektionsindex}


\end{document}
